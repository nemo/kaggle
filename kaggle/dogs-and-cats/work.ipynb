{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG16 to solve Kaggle's Dogs vs Cats problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/kaggle/dogs-and-cats/data/test\n"
     ]
    }
   ],
   "source": [
    "%cd data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mkdir unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/kaggle/dogs-and-cats/data/train\n"
     ]
    }
   ],
   "source": [
    "%cd ../train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "!mv cat.* cats\n",
    "!mv dog.* dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/kaggle/dogs-and-cats/data\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "%mkdir valid\n",
    "%mkdir valid/dogs\n",
    "%mkdir valid/cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "N=1000;\n",
    "for i in ./train/dogs/*; do\n",
    "  [ $((N--)) = 0 ] && break\n",
    "  cp -t \"./valid/dogs\" -- \"$i\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "N=1000;\n",
    "for i in ./train/cats/*; do\n",
    "  [ $((N--)) = 0 ] && break\n",
    "  cp -t \"./valid/cats\" -- \"$i\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path = \"\"\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A few basic libraries that we'll need for the initial exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json, csv\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a pretrained VGG model with our **Vgg16** class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is simply to use a model that has been fully created for us, which can recognise a wide variety (1,000 categories) of images. We will use 'VGG', which won the 2014 Imagenet competition, and is a very simple model to create and understand. The VGG Imagenet team created both a larger, slower, slightly more accurate model (*VGG  19*) and a smaller, faster model (*VGG 16*). We will be using VGG 16 since the much slower performance of VGG19 is generally not worth the very minor improvement in accuracy.\n",
    "\n",
    "We have created a python class, *Vgg16*, which makes using the VGG 16 model very straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The punchline: state of the art custom model in 7 lines of code\n",
    "\n",
    "Here's everything you need to do to get >97% accuracy on the Dogs vs Cats dataset - we won't analyze how it works behind the scenes yet, since at this stage we're just going to focus on the minimum necessary to actually do useful work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/nima/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# Import our class, and instantiate\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Vgg16 for basic image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size)\n",
    "test_batches = vgg.get_batches(path+'test', batch_size=batch_size, shuffle=False)\n",
    "file_ids = list(map(lambda x: int(x.split(\"/\")[1].replace(\".jpg\", \"\")), test_batches.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 500s - loss: 2.2321 - acc: 0.8598 - val_loss: 0.6495 - val_acc: 0.9580\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights(path+'results/ft2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(path+'results/ft2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 499s - loss: 0.4847 - acc: 0.9693 - val_loss: 0.3356 - val_acc: 0.9790\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.01\n",
    "vgg.fit(batches, val_batches, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights(path+'results/ft3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = vgg.model.predict_generator(test_batches, len(test_batches.filenames), max_q_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvfile = open('submission.csv', 'w')\n",
    "writer = csv.writer(csvfile, delimiter=\",\")\n",
    "\n",
    "preds = predictions[:]\n",
    "preds = np.delete(preds, 0, axis=1)\n",
    "preds = np.clip(preds, 0.05, 0.96)\n",
    "data = np.column_stack((file_ids, preds.flatten()))\n",
    "\n",
    "np.savetxt(\"submission.csv\", data, fmt=['%d', '%.18f'], delimiter=\",\", header=\"id,label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/nima/workspace/kaggle/dogs-and-cats/submission.csv"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "FileLink(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvfile = open('submission.csv', 'w')\n",
    "writer = csv.writer(csvfile, delimiter=\",\")\n",
    "index = 0\n",
    "batch_count = 0\n",
    "results = []\n",
    "print(\"total length: %d\" % (len(file_ids)))\n",
    "\n",
    "batch,labels = next(test_batches, None)\n",
    "while(batch is not None and len(batch) > 0 and index < len(file_ids)):\n",
    "    preds, idxs, labels = vgg.predict(batch, False)\n",
    "    for img_class in idxs:\n",
    "        writer.writerow([file_ids[index], img_class])b\n",
    "        index = index + 1\n",
    "    batch_count = batch_count + 1\n",
    "    batch,labels = next(test_batches, None)\n",
    "\n",
    "print(\"total batches: %s\" % (batch_count, index))\n",
    "csvfile.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
